# CAPCA

This is a draft, intended to be submitted as a legislative proposal for  [Legislative Proposal Request | Official Website - Assemblyman Rob Bonta Representing the 18th California Assembly District](https://a18.asmdc.org/legislative-proposal-request)


## Proposal Summary - Please describe the proposal in one sentence 

Require large social media companies to notify advertisers when their
ads are shown on illegal or policy-violating content.

## Problem - Please describe the problem(s) that the proposal would address (please be specific, with supporting data and sources)

Today, social media companies
choose to host a [disturbing variety of illegal
activity](https://www.wsj.com/articles/the-worst-job-in-technology-staring-at-human-depravity-to-keep-it-off-facebook-1514398398),
including

 * [Foreign election disinformation](https://www.usatoday.com/story/news/2018/05/11/what-we-found-facebook-ads-russians-accused-election-meddling/602319002/)

 * [domestic terrorist event information](https://www.buzzfeednews.com/article/ryanmac/facebook-moderators-call-arms-not-enforced-kenosha)

 * [international terrorist recruiting material](https://apnews.com/article/f97c24dab4f34bd0b48b36f2988952a4)

Besides the high-profile political controversy that these large companies are best known for,
social media platforms also host ordinary crimes, including

 * [child abuse material](https://www.foxglove.org.uk/news/open-letter-from-content-moderators-re-pandemic)

 * [personal finance scams](https://www.bbc.com/news/technology-46972940)

Social media companies choose to under-invest in implementation of the
law, and even of their own policies, because they can get away with it.
Social media services are designed with "filter bubbles" to show
specific content to specific individuals. 

Although the main victims of social media practices are users, the under-rated
victims are the advertisers who unknowingly pay to sponsor
illegal activity.  Filter bubbles enable social media companies to sell
a secretly adulterated product&mdash;ad placements on activity that
the advertiser would not choose to sponsor if aware of it.

Advertisers seldom choose to sponsor illegal activity
and typically pull their ads when they do discover it.
However, social media companies are failing to inform
advertisers when they sponsor illegal activity, even
when advertisers are clearly taking reputational risks
on content that the social media companies themselves
will not associate with their end-user-facing brands.

On social media sites, it is impractical or prohibited
by terms of service for
advertisers, especially small businesses, to monitor
every piece of content that their ads support. Ads
and content are connected automatically and users may
see the same ad in association with many different
pieces of content.

Advertisers pay the bills for illegal activity because
they lack the information they would need to stop
doing so. 

## Solution - Please describe the proposal and how the proposal would address the problem (please be specific, citing existing law if possible)

When content is removed or limited in circulation,
a social media platform must
disclose to the advertisers affected:

 * What content was removed

 * On what grounds the content was removed

 * How long the content stayed up

 * How many users were affected, if known

 * The amount that the advertiser was charged

If the illegal content was another advertisement,
disclose to nearby advertisers.  For example, if a
real mortgage broker had their ad run along with an ad
for a cryptocurrency scam, and the cryptocurrency scam
ad is removed, the mortgage broker should be informed.

Disclosure should include violations of laws that
apply to the affected users, even if they do not
apply to the advertisers.   For example, if a
California advertiser's ad ends up on a video that
is not allowed to be displayed in Germany, and the
ad is shown to users in Germany before the video is
blocked there, then the California advertiser should
be notified even if the video is still available to
users in California.  This is about protecting a brand
owner's reputation as seen by the audience they are
paying to reach, not about the speech-related laws
of particular countries.

This disclosure would be similar
to the existing [Data Security Breach
Reporting](https://oag.ca.gov/privacy/databreach/reporting)
required under  (California Civil Code s. 1798.29(a)
[agency] and California Civ. Code s. 1798.82(a).



## Cost - Please describe the estimated cost of proposal and identify the entity that would pay for the proposal. If state would pay, please identify a source for the funding and where you would recommend cutting state spending to pay for the proposal.

The costs of this proposal would be paid by the
large social media companies.  This proposal would
apply to a small number of firms: any companies that both
distribute user-generated content from millions of
California users and that accept advertising from
thousands of California advertisers.

The cost of compliance would be minimal, as social
media companies already have the information that
they would be required to disclose, for their own
internal reporting purposes.  And the companies already
maintain a reporting interface for their advertisers.
The proposal could be implemented as simply as by
adding an HTML table and a text notification to an
existing "advertiser dashboard" web application.

The actual cost for implementation by each firm
affected would be under $1 million.

After social media companies begin the required
disclosures under this proposal, they would likely
have to accelerate their existing plans to fix
under-resourced moderation and ad review. The cost
of this follow-on work would depend on a variety of
compensation decisions, but would likely involve
hiring of technical, policy, and subject matter
experts in the state of California.


## Organizational Support - Please describe the likely organizations that would support the proposal.

 * Law enforcement. Incentivizing the early removal of illegal material
   would help prevent social-media-enabled crime that can otherwise become
   a problem at the local level. (For example, [private gun sales](https://www.protocol.com/facebook-gun-sales-ban) on social media can result in weapons becoming available to those
   who cannot legally own them.)

 * Small business.  Many local retailers and real estate agents have a difficult
   choice. If they avoid sponsoring criminal activity by refraining from advertising
   on social media, they must give up an easy self-serve tool to reach local customers.
   Disclosure would enable each business decision maker to buy advertising based on
   their own comfort level

## Arguments in Support - Please identify the top three arguments in support of the proposal.

 * Reporting protects First Amendment rights of
   social media users and advertisers. Some
   advertisers may choose to continue to support
   controversial material, or material that is
   later removed for legal reasons.  For example,
   [fans of the rap group Insane Clown Posse were
   listed as a gang by the National Gang Intelligence
   Center](https://en.wikipedia.org/wiki/Insane_Clown_Posse#2013%E2%80%9314:_Continued_legal_troubles).
   This listing could result in policy violations
   and content deletions affecting those fans on
   social sites.  If an advertiser chooses not to
   take action on notifications and continues to
   support content related to Insane Clown Posse,
   no new law should interfere with their ability to
   do so.  Social platforms also remove or restrict
   some content in error, including content that
   advertisers would want to continue to support.

 * Accountability is good for public health.
   Much of the material that is eventually removed from social media
   sites turns out to be either an ordinary scam based on people's health concerns,
   or active disinformation about a health-related conspiracy theory.

 * Because this proposal is simple and could take effect quickly, it can
   help inform other work by advertisers, social media companies,
   and policymakers.  While questions of how best to operate
   and moderate social media sites are still actively researched,
   this proposal provides a simple way to keep stakeholders informed on the state of
   progress.


## Organizational Opposition - Please describe the likely organizations that would oppose the proposal.

 * Large social media companies, which are currently able to profit from
   illegal activity using ad revenue from advertisers who are not aware
   of where their ads appear.

 * Foreign disinformation groups, which benefit from under-moderation of
   large social sites to spread violence and disease in the USA.



## Arguments in Opposition - Please identify the top three arguments in opposition to the proposal.

 * Disclosure will distract social media companies from other law and policy enforcement
   projects.  **See the costs section. This proposal requires only a small amount
   of work, to expose to advertisers a set of data that already exists within
   social media companies.  It would likely be carried out by an advertiser reporting
   team, not the developers who work on moderation and public interest projects.**

 * Advertisers who are concerned about the environment
   in which their ads appear can
   already use ad transparency tools such as
   NYU Ad Observatory.  **This proposal would
   complement, not replace, transparency tools,
   and would remain effective even if [social
   media companies could force such tools to shut
   down](https://www.wsj.com/articles/facebook-seeks-shutdown-of-nyu-research-project-into-political-ad-targeting-11603488533).**

 * Advertisers have not demanded these reports yet.
   **Social media crime and disinformation is an issue of growing
   concern, as shown by advertiser boycott activity this summer.
   Advertisers who choose not to act on the reports required by
   this proposal would not have to.**


# Additional material

 * How to define an association between ad and content: borrow viewability standards from [IAB Measurement Guidelines](https://www.iab.com/guidelines/iab-measurement-guidelines/)?

 * Related: [RECOMMENDED NEXT STEPS | Stop Hate for Profit](https://www.stophateforprofit.org/productrecommendations) <q>Provide audit of and refund to advertisers whose ads were shown next to content that was later removed for violations of terms of service.</q>

